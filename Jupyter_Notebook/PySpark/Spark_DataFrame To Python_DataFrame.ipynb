{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting Date to Spark DataFrame and to Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of Dept: <class 'list'>\n",
      "Type of RDD: <class 'pyspark.rdd.RDD'>\n",
      "root\n",
      " |-- dept_name: string (nullable = true)\n",
      " |-- dept_id: long (nullable = true)\n",
      "\n",
      "+---------+-------+\n",
      "|dept_name|dept_id|\n",
      "+---------+-------+\n",
      "|Finance  |10     |\n",
      "|Marketing|20     |\n",
      "|Sales    |30     |\n",
      "|IT       |40     |\n",
      "+---------+-------+\n",
      "\n",
      "root\n",
      " |-- dept_name: string (nullable = true)\n",
      " |-- dept_id: string (nullable = true)\n",
      "\n",
      "+---------+-------+\n",
      "|dept_name|dept_id|\n",
      "+---------+-------+\n",
      "|Finance  |10     |\n",
      "|Marketing|20     |\n",
      "|Sales    |30     |\n",
      "|IT       |40     |\n",
      "+---------+-------+\n",
      "\n",
      "   dept_name dept_id\n",
      "0    Finance      10\n",
      "1  Marketing      20\n",
      "2      Sales      30\n",
      "3         IT      40\n",
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType,StructField, StringType\n",
    "\n",
    "# Creating the SparkSession\n",
    "spark = SparkSession.builder.appName('test').master(\"local\").getOrCreate()\n",
    "\n",
    "# Creating a list \n",
    "dept = [(\"Finance\",10),(\"Marketing\",20),(\"Sales\",30),(\"IT\",40)]\n",
    "print('Type of Dept:', type(dept))\n",
    "\n",
    "# Converting the list to RDD\n",
    "rdd = spark.sparkContext.parallelize(dept)\n",
    "print('Type of RDD:', type(rdd))\n",
    "\n",
    "# Converting RDD to Spark Dataframe\n",
    "df = rdd.toDF()\n",
    "# print(df.printSchema())\n",
    "# print('Print the DataFrame:', df.show())\n",
    "# print('Print the DF Truncate is false:', df.show(truncate=False))\n",
    "\n",
    "deptColumns = [\"dept_name\",\"dept_id\"]\n",
    "df2 = rdd.toDF(deptColumns)\n",
    "# df2.printSchema()\n",
    "# print(df2.show())\n",
    "# df2.show(truncate=False)\n",
    "\n",
    "# Creating DataFrame using createdataframe()\n",
    "deptDF = spark.createDataFrame(rdd, schema = deptColumns)\n",
    "deptDF.printSchema()\n",
    "deptDF.show(truncate=False)\n",
    "\n",
    "deptSchema = StructType([\n",
    "    StructField('dept_name', StringType(), True),\n",
    "    StructField('dept_id', StringType(), True)])\n",
    "\n",
    "deptDF1 = spark.createDataFrame(rdd, schema = deptSchema)\n",
    "deptDF1.printSchema()\n",
    "deptDF1.show(truncate=False)\n",
    "\n",
    "# Convert the Spark DataFrame back to a pandas DataFrame using Arrow\n",
    "result_pdf = deptDF1.select(\"*\").toPandas()\n",
    "print(result_pdf)\n",
    "print(type(result_pdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas_DataFrame to Spark_DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+-------------------+\n",
      "|                  0|                  1|                  2|\n",
      "+-------------------+-------------------+-------------------+\n",
      "| 0.3808938518514766|0.24926725694913243| 0.7876664096198096|\n",
      "|   0.67340984507219|0.09393267818780915|0.17317978907063503|\n",
      "| 0.6122485744219693| 0.7089185452832107| 0.6977537865821086|\n",
      "| 0.9649373422850355|0.02949278375756803| 0.7009717218583225|\n",
      "|0.21491857683971338| 0.1947534238002514| 0.6050142280044305|\n",
      "| 0.3963480711496761|0.25348755637772313| 0.2932977515942553|\n",
      "|  0.311324988383484| 0.7909107193493403|0.39680161417902027|\n",
      "|0.05436951351086594| 0.8068657712936341| 0.8645587328424736|\n",
      "| 0.5371481810262208|0.02548339847711878| 0.4803846945606256|\n",
      "| 0.5635897908186178|0.05685372945364697|0.07251783812343349|\n",
      "+-------------------+-------------------+-------------------+\n",
      "\n",
      "None\n",
      "TYpe of df: <class 'pyspark.sql.dataframe.DataFrame'>\n",
      "          0         1         2\n",
      "0  0.380894  0.249267  0.787666\n",
      "1  0.673410  0.093933  0.173180\n",
      "2  0.612249  0.708919  0.697754\n",
      "3  0.964937  0.029493  0.700972\n",
      "4  0.214919  0.194753  0.605014\n",
      "5  0.396348  0.253488  0.293298\n",
      "6  0.311325  0.790911  0.396802\n",
      "7  0.054370  0.806866  0.864559\n",
      "8  0.537148  0.025483  0.480385\n",
      "9  0.563590  0.056854  0.072518\n",
      "TYpe of result_pdf: <class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Creating Spark Session\n",
    "spark = SparkSession.builder.appName(\"SparkDFtoPandasDF\").master(\"local\").getOrCreate()\n",
    "\n",
    "# Enable Arrow-based columnar data transfers\n",
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "\n",
    "# Generate a pandas DataFrame\n",
    "pdf = pd.DataFrame(np.random.rand(10, 3))\n",
    "# print(pdf)\n",
    "\n",
    "# Create a Spark DataFrame from a Pandas DataFrame using Arrow\n",
    "sdf = spark.createDataFrame(pdf)\n",
    "print(sdf.show())\n",
    "print('TYpe of df:', type(sdf))\n",
    "\n",
    "# Convert the Spark DataFrame back to a pandas DataFrame using Arrow\n",
    "result_pdf = sdf.select(\"*\").toPandas()\n",
    "print(result_pdf)\n",
    "print('TYpe of result_pdf:', type(result_pdf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    OrderDate   Region       Rep     Item  Units  UnitCost     Total\n",
      "0    1/6/2018     East     Jones   Pencil     95      1.99    189.05\n",
      "1   1/23/2018  Central    Kivell   Binder     50     19.99     999.5\n",
      "2    2/9/2018  Central   Jardine   Pencil     36      4.99    179.64\n",
      "3   2/26/2018  Central      Gill      Pen     27     19.99    539.73\n",
      "4   3/15/2018     West   Sorvino   Pencil     56      2.99    167.44\n",
      "5    4/1/2018     East     Jones   Binder     60      4.99     299.4\n",
      "6   4/18/2018  Central   Andrews   Pencil     75      1.99    149.25\n",
      "7    5/5/2018  Central   Jardine   Pencil     90      4.99     449.1\n",
      "8   5/22/2018     West  Thompson   Pencil     32      1.99     63.68\n",
      "9    6/8/2018     East     Jones   Binder     60      8.99     539.4\n",
      "10  6/25/2018  Central    Morgan   Pencil     90      4.99     449.1\n",
      "11  7/12/2018     East    Howard   Binder     29      1.99     57.71\n",
      "12  7/29/2018     East    Parent   Binder     81     19.99  1,619.19\n",
      "13  8/15/2018     East     Jones   Pencil     35      4.99    174.65\n",
      "14   9/1/2018  Central     Smith     Desk      2    125.00       250\n",
      "15  9/18/2018     East     Jones  Pen Set     16     15.99    255.84\n",
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.master('local').appName('DataFrame_Hdfs').getOrCreate()\n",
    "data_df = spark.read.csv('C:\\\\Project\\\\Files\\\\Input\\\\csv\\\\Sample.csv' ,inferSchema=True, header=True)\n",
    "\n",
    "# data_df = spark.read.format('csv').options(inferSchema=True, header=True,delimiter=',').csv('C:\\\\Project\\\\Files\\\\Input\\\\csv\\\\Sample.csv')\n",
    "\n",
    "# print(data_df.show())\n",
    "\n",
    "# print('Schema:',data_df.printSchema())\n",
    "# print('The partition isze:',data_df.rdd.getNumPartitions())\n",
    "\n",
    "# print('Default Parallelism:',spark.sparkContext.defaultParallelism)\n",
    "\n",
    "pdf = data_df.select(\"*\").toPandas()\n",
    "print(pdf)\n",
    "print(type(pdf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
