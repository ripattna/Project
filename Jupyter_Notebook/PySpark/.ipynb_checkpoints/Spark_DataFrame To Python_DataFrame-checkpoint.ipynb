{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting Date to Spark DataFrame and then to Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of Dept: <class 'list'>\n",
      "Type of RDD: <class 'pyspark.rdd.RDD'>\n",
      "root\n",
      " |-- dept_name: string (nullable = true)\n",
      " |-- dept_id: long (nullable = true)\n",
      "\n",
      "+---------+-------+\n",
      "|dept_name|dept_id|\n",
      "+---------+-------+\n",
      "|Finance  |10     |\n",
      "|Marketing|20     |\n",
      "|Sales    |30     |\n",
      "|IT       |40     |\n",
      "+---------+-------+\n",
      "\n",
      "root\n",
      " |-- dept_name: string (nullable = true)\n",
      " |-- dept_id: string (nullable = true)\n",
      "\n",
      "+---------+-------+\n",
      "|dept_name|dept_id|\n",
      "+---------+-------+\n",
      "|Finance  |10     |\n",
      "|Marketing|20     |\n",
      "|Sales    |30     |\n",
      "|IT       |40     |\n",
      "+---------+-------+\n",
      "\n",
      "   dept_name dept_id\n",
      "0    Finance      10\n",
      "1  Marketing      20\n",
      "2      Sales      30\n",
      "3         IT      40\n",
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType,StructField, StringType\n",
    "\n",
    "# Creating the SparkSession\n",
    "spark = SparkSession.builder.appName('test').master(\"local\").getOrCreate()\n",
    "\n",
    "# Creating a list \n",
    "dept = [(\"Finance\",10),(\"Marketing\",20),(\"Sales\",30),(\"IT\",40)]\n",
    "print('Type of Dept:', type(dept))\n",
    "\n",
    "# Converting the list to RDD\n",
    "rdd = spark.sparkContext.parallelize(dept)\n",
    "print('Type of RDD:', type(rdd))\n",
    "\n",
    "# Converting RDD to Spark Dataframe\n",
    "df = rdd.toDF()\n",
    "# print(df.printSchema())\n",
    "# print('Print the DataFrame:', df.show())\n",
    "# print('Print the DF Truncate is false:', df.show(truncate=False))\n",
    "\n",
    "deptColumns = [\"dept_name\",\"dept_id\"]\n",
    "df2 = rdd.toDF(deptColumns)\n",
    "# df2.printSchema()\n",
    "# print(df2.show())\n",
    "# df2.show(truncate=False)\n",
    "\n",
    "# Creating DataFrame using createdataframe()\n",
    "deptDF = spark.createDataFrame(rdd, schema = deptColumns)\n",
    "deptDF.printSchema()\n",
    "deptDF.show(truncate=False)\n",
    "\n",
    "deptSchema = StructType([\n",
    "    StructField('dept_name', StringType(), True),\n",
    "    StructField('dept_id', StringType(), True)])\n",
    "\n",
    "deptDF1 = spark.createDataFrame(rdd, schema = deptSchema)\n",
    "deptDF1.printSchema()\n",
    "deptDF1.show(truncate=False)\n",
    "\n",
    "# Convert the Spark DataFrame back to a pandas DataFrame using Arrow\n",
    "result_pdf = deptDF1.select(\"*\").toPandas()\n",
    "print(result_pdf)\n",
    "print(type(result_pdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0         1         2\n",
      "0  0.359982  0.413753  0.879132\n",
      "1  0.863343  0.795376  0.388219\n",
      "2  0.086843  0.809729  0.011066\n",
      "3  0.656084  0.345846  0.514123\n",
      "4  0.161523  0.639518  0.679849\n",
      "5  0.236000  0.312934  0.322354\n",
      "6  0.793412  0.624603  0.760949\n",
      "7  0.202881  0.473225  0.555773\n",
      "8  0.554594  0.288513  0.623083\n",
      "9  0.313588  0.143480  0.267848\n",
      "TYpe of df: <class 'pyspark.sql.dataframe.DataFrame'>\n",
      "          0         1         2\n",
      "0  0.359982  0.413753  0.879132\n",
      "1  0.863343  0.795376  0.388219\n",
      "2  0.086843  0.809729  0.011066\n",
      "3  0.656084  0.345846  0.514123\n",
      "4  0.161523  0.639518  0.679849\n",
      "5  0.236000  0.312934  0.322354\n",
      "6  0.793412  0.624603  0.760949\n",
      "7  0.202881  0.473225  0.555773\n",
      "8  0.554594  0.288513  0.623083\n",
      "9  0.313588  0.143480  0.267848\n",
      "TYpe of result_pdf: <class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Creating Spark Session\n",
    "spark = SparkSession.builder.appName(\"SparkDFtoPandasDF\").master(\"local\").getOrCreate()\n",
    "\n",
    "# Enable Arrow-based columnar data transfers\n",
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "\n",
    "# Generate a pandas DataFrame\n",
    "pdf = pd.DataFrame(np.random.rand(10, 3))\n",
    "print(pdf)\n",
    "\n",
    "# Create a Spark DataFrame from a Pandas DataFrame using Arrow\n",
    "df = spark.createDataFrame(pdf)\n",
    "print('TYpe of df:', type(df))\n",
    "\n",
    "# Convert the Spark DataFrame back to a pandas DataFrame using Arrow\n",
    "result_pdf = df.select(\"*\").toPandas()\n",
    "print(result_pdf)\n",
    "print('TYpe of result_pdf:', type(result_pdf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.master('local').appName('DataFrame_Hdfs').getOrCreate()\n",
    "data_df = spark.read.csv('C:\\\\Project\\\\Files\\\\Input\\\\csv\\\\Sample.csv' ,inferSchema=True, header=True)\n",
    "\n",
    "# data_df = spark.read.format('csv').options(inferSchema=True, header=True,delimiter=',').csv('C:\\\\Project\\\\Files\\\\Input\\\\csv\\\\Sample.csv')\n",
    "\n",
    "print(data_df.show())\n",
    "\n",
    "print('Schema:',data_df.printSchema())\n",
    "# print('The partition isze:',data_df.rdd.getNumPartitions())\n",
    "\n",
    "# print('Default Parallelism:',spark.sparkContext.defaultParallelism)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
