{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\BigData\\Spark\\python\\pyspark\\sql\\pandas\\conversion.py:289: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:\n",
      "  PyArrow >= 0.15.1 must be installed; however, it was not found.\n",
      "Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.\n",
      "  warnings.warn(msg)\n",
      "C:\\BigData\\Spark\\python\\pyspark\\sql\\pandas\\conversion.py:88: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:\n",
      "  PyArrow >= 0.15.1 must be installed; however, it was not found.\n",
      "Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           0         1         2\n",
      "0   0.574592  0.222611  0.961859\n",
      "1   0.309749  0.028956  0.976699\n",
      "2   0.045204  0.819235  0.385705\n",
      "3   0.487385  0.390174  0.585341\n",
      "4   0.302630  0.614447  0.459364\n",
      "..       ...       ...       ...\n",
      "95  0.587732  0.148356  0.030290\n",
      "96  0.931934  0.240875  0.322710\n",
      "97  0.068133  0.051549  0.631551\n",
      "98  0.940765  0.527513  0.529970\n",
      "99  0.298443  0.500501  0.272484\n",
      "\n",
      "[100 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Creating Spark Session\n",
    "spark = SparkSession.builder.appName(\"SparkDFtoPandasDF\").master(\"local\").getOrCreate()\n",
    "\n",
    "# Enable Arrow-based columnar data transfers\n",
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "\n",
    "# Generate a pandas DataFrame\n",
    "pdf = pd.DataFrame(np.random.rand(100, 3))\n",
    "\n",
    "# Create a Spark DataFrame from a pandas DataFrame using Arrow\n",
    "df = spark.createDataFrame(pdf)\n",
    "\n",
    "# Convert the Spark DataFrame back to a pandas DataFrame using Arrow\n",
    "result_pdf = df.select(\"*\").toPandas()\n",
    "print(result_pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
